{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OpyvweRhhhQ4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synonym replacement\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "    new_words = words.copy()\n",
        "    #stopwords = set(stopwords.words('english'))\n",
        "    random_word_list = list(set([word for word in words if word not in stopwords_set]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n: \n",
        "            break\n",
        "    sentence = ' '.join(new_words)\n",
        "    new_words = sentence.split(' ')\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word): \n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "            synonyms.add(synonym) \n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "# Random deletion\n",
        "def random_deletion(words, p):\n",
        "    if len(words) == 1:\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0:\n",
        "        return [random.choice(words)]\n",
        "    else:\n",
        "        return remaining\n",
        "\n",
        "# Random swap\n",
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "kBmngcEyhj4C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paIFrlprkvry",
        "outputId": "36fb6a00-840b-4369-cc42-fbb9491eba69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline\n",
        "\n",
        "def augment_sentence(sentence, num_new_sentences=5):\n",
        "    augmented_sentences = []\n",
        "    words = sentence.split(' ')\n",
        "    for _ in range(num_new_sentences):\n",
        "        augmented = words\n",
        "        function_list = [synonym_replacement, random_deletion, random_swap]\n",
        "        random.shuffle(function_list)\n",
        "        for function in function_list:\n",
        "            if function.__name__ == 'synonym_replacement':\n",
        "                augmented = function(augmented, n=2)\n",
        "            elif function.__name__ == 'random_deletion':\n",
        "                augmented = function(augmented, p=0.1)\n",
        "            elif function.__name__ == 'random_swap':\n",
        "                augmented = function(augmented, n=2)\n",
        "        augmented_sentences.append(' '.join(augmented))\n",
        "    return augmented_sentences\n"
      ],
      "metadata": {
        "id": "MPl_eI9uiCwJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Avoid if preprocessing will be done, I added this since this is a generic template.\n",
        "\n",
        "import string\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    # Remove punctuation\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "l9HjvQHXpTiO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['This is the first.', 'Here is another one.', 'This is the third sentence.']\n",
        "\n",
        "augmented_data = []\n",
        "for sentence in data:\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    augmented_data.extend(augment_sentence(sentence))\n",
        "\n",
        "print(augmented_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLqJ5yUkiIBk",
        "outputId": "a8399870-3172-425f-91a8-afbf76cda5ec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this the time for the first is', 'offset the this', 'is foremost the this', 'this is the kickoff', 'first class honours degree is this', 'other is unmatchable here', 'other single is some', 'some other is here', 'other here some', 'some peerless here is', 'is the this tierce condemnation', 'is thirdly the this prison term', 'this is the thirdly prison term', 'is the this one third conviction', 'this is one the doom third']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoYiQYZTlDV8",
        "outputId": "e52f14a6-8f33-4f4d-a74a-f009e9421526"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}